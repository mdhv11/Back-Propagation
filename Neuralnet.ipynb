{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('heart_statlog_cleveland_hungary_final.csv')\n",
    "\n",
    "# Checking for missing values\n",
    "print(\"Missing values in each column:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Fill missing values with the mean of the column\n",
    "data.fillna(data.mean(), inplace=True)\n",
    "\n",
    "# Display the first few rows of the dataset to understand its structure\n",
    "print(data.head())\n",
    "\n",
    "# Assuming 'sex', 'chest pain type', 'fasting blood sugar', 'resting ecg', 'exercise angina', and 'ST slope' are categorical\n",
    "categorical_columns = ['sex', 'chest pain type', 'fasting blood sugar', 'resting ecg', 'exercise angina', 'ST slope']\n",
    "\n",
    "# Converting categorical columns to numerical format using one-hot encoding\n",
    "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Ensure all data is numeric\n",
    "data = data.apply(pd.to_numeric)\n",
    "\n",
    "# Detecting and handling outliers\n",
    "# Determine the number of rows and columns for the subplots\n",
    "num_columns = len(data.columns)\n",
    "num_rows = (num_columns + 3) // 4  # Adjust for 4 columns per row\n",
    "\n",
    "# Plotting boxplots to visualize outliers\n",
    "plt.figure(figsize=(15, num_rows * 3))\n",
    "for i, column in enumerate(data.columns, 1):\n",
    "    plt.subplot(num_rows, 4, i)\n",
    "    sns.boxplot(data[column])\n",
    "    plt.title(column)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Removing outliers using Z-score method\n",
    "z_scores = np.abs(stats.zscore(data.select_dtypes(include=[np.number])))\n",
    "data = data[(z_scores < 3).all(axis=1)]\n",
    "\n",
    "# Separating features and target variable\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Confirm the shapes of the train and test sets\n",
    "print(f\"Training set shape: {X_train.shape}, Testing set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Initialize the model\n",
    "mlr = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "mlr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_mlr = mlr.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_mlr = mean_squared_error(y_test, y_pred_mlr)\n",
    "print(f'Mean Squared Error for MLR-F: {mse_mlr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 0.25002213468882206, Validation Loss: 0.25001223380457177\n",
      "Early stopping at epoch 10\n",
      "Epoch 0, Train Loss: 0.2500279690106443, Validation Loss: 0.2500082525178327\n",
      "Early stopping at epoch 10\n",
      "Epoch 0, Train Loss: 0.2500121556307631, Validation Loss: 0.2500720051405847\n",
      "Early stopping at epoch 0\n",
      "Epoch 0, Train Loss: 0.2500333104605538, Validation Loss: 0.25000993559250806\n",
      "Early stopping at epoch 0\n",
      "Validation Error: 0.25002643017062937\n",
      "Mean Squared Error for BP: 0.2500123076115742\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class NeuralNet:\n",
    "    def __init__(self, layers, epochs=1000, learning_rate=0.01, momentum=0.9, activation='sigmoid', validation_split=0.2, patience=10, batch_size=32):\n",
    "        \"\"\"\n",
    "        Initialize the neural network with the given parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        - layers: List of integers, the number of neurons in each layer.\n",
    "        - epochs: Integer, the number of epochs for training.\n",
    "        - learning_rate: Float, the learning rate for gradient descent.\n",
    "        - momentum: Float, the momentum factor for gradient descent.\n",
    "        - activation: String, the activation function to use ('sigmoid', 'relu', 'linear', 'tanh').\n",
    "        - validation_split: Float, the proportion of data to use for validation.\n",
    "        - patience: Integer, the number of epochs to wait for improvement before early stopping.\n",
    "        - batch_size: Integer, the number of samples per batch.\n",
    "        \"\"\"\n",
    "        self.L = len(layers)\n",
    "        self.n = layers\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.fact = activation\n",
    "        self.validation_split = validation_split\n",
    "        self.patience = patience\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.h = [np.zeros((layer, 1)) for layer in layers]\n",
    "        self.xi = [np.zeros((layer, 1)) for layer in layers]\n",
    "        self.w = [np.random.randn(layers[i], layers[i-1]) * 0.01 if i > 0 else np.zeros((1, 1)) for i in range(self.L)]\n",
    "        self.theta = [np.zeros((layer, 1)) for layer in layers]\n",
    "        self.delta = [np.zeros((layer, 1)) for layer in layers]\n",
    "        self.d_w = [np.zeros_like(w) for w in self.w]\n",
    "        self.d_theta = [np.zeros_like(theta) for theta in self.theta]\n",
    "        self.d_w_prev = [np.zeros_like(w) for w in self.w]\n",
    "        self.d_theta_prev = [np.zeros_like(theta) for theta in self.theta]\n",
    "\n",
    "        self.activation_function = self.get_activation_function(activation)\n",
    "        self.activation_derivative = self.get_activation_derivative(activation)\n",
    "\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def get_activation_function(self, name):\n",
    "        \"\"\"Return the activation function based on the given name.\"\"\"\n",
    "        if name == 'sigmoid':\n",
    "            return lambda z: 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "        elif name == 'relu':\n",
    "            return lambda z: np.maximum(0, z)\n",
    "        elif name == 'linear':\n",
    "            return lambda z: z\n",
    "        elif name == 'tanh':\n",
    "            return lambda z: np.tanh(z)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "    def get_activation_derivative(self, name):\n",
    "        \"\"\"Return the derivative of the activation function based on the given name.\"\"\"\n",
    "        if name == 'sigmoid':\n",
    "            return lambda z: z * (1 - z)\n",
    "        elif name == 'relu':\n",
    "            return lambda z: np.where(z > 0, 1, 0)\n",
    "        elif name == 'linear':\n",
    "            return lambda z: np.ones_like(z)\n",
    "        elif name == 'tanh':\n",
    "            return lambda z: 1 - z**2\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        \"\"\"Perform forward propagation through the network.\"\"\"\n",
    "        self.xi[0] = X\n",
    "        for lay in range(1, self.L):\n",
    "            self.h[lay] = np.dot(self.w[lay], self.xi[lay - 1]) - self.theta[lay]\n",
    "            self.xi[lay] = self.activation_function(self.h[lay])\n",
    "        return self.xi[-1]\n",
    "\n",
    "    def backward_propagation(self, X, y):\n",
    "        \"\"\"Perform backward propagation through the network.\"\"\"\n",
    "        m = y.shape[1]\n",
    "        self.forward_propagation(X)\n",
    "        self.delta[-1] = (self.xi[-1] - y) * self.activation_derivative(self.h[-1])\n",
    "\n",
    "        for lay in range(self.L - 2, 0, -1):\n",
    "            self.delta[lay] = self.activation_derivative(self.h[lay]) * np.dot(self.w[lay + 1].T, self.delta[lay + 1])\n",
    "\n",
    "        for lay in range(1, self.L):\n",
    "            self.d_w[lay] = np.dot(self.delta[lay], self.xi[lay - 1].T) / m\n",
    "            self.d_theta[lay] = np.sum(self.delta[lay], axis=1, keepdims=True) / m\n",
    "\n",
    "            self.d_w_prev[lay] = self.momentum * self.d_w_prev[lay] - self.learning_rate * self.d_w[lay]\n",
    "            self.d_theta_prev[lay] = self.momentum * self.d_theta_prev[lay] - self.learning_rate * self.d_theta[lay]\n",
    "\n",
    "            self.w[lay] += self.d_w_prev[lay]\n",
    "            self.theta[lay] += self.d_theta_prev[lay]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train the neural network using the training data.\"\"\"\n",
    "        kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "        validation_errors = []\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "\n",
    "        for train_index, val_index in kf.split(X.T):\n",
    "            X_train, X_val = X[:, train_index], X[:, val_index]\n",
    "            y_train, y_val = y[:, train_index], y[:, val_index]\n",
    "\n",
    "            for epoch in range(self.epochs):\n",
    "                for i in range(0, X_train.shape[1], self.batch_size):\n",
    "                    end = i + self.batch_size\n",
    "                    self.backward_propagation(X_train[:, i:end], y_train[:, i:end])\n",
    "                \n",
    "                if epoch % 100 == 0:\n",
    "                    train_loss = np.mean((self.forward_propagation(X_train) - y_train) ** 2)\n",
    "                    val_loss = np.mean((self.forward_propagation(X_val) - y_val) ** 2)\n",
    "                    self.train_losses.append(train_loss)\n",
    "                    self.val_losses.append(val_loss)\n",
    "                    print(f'Epoch {epoch}, Train Loss: {train_loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "                # Early stopping\n",
    "                val_loss = np.mean((self.forward_propagation(X_val) - y_val) ** 2)\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "\n",
    "                if patience_counter >= self.patience:\n",
    "                    print(f'Early stopping at epoch {epoch}')\n",
    "                    break\n",
    "\n",
    "            val_error = np.mean((self.forward_propagation(X_val) - y_val) ** 2)\n",
    "            validation_errors.append(val_error)\n",
    "\n",
    "        self.validation_error = np.mean(validation_errors)\n",
    "        print(f'Validation Error: {self.validation_error}')\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions using the trained neural network.\"\"\"\n",
    "        return self.forward_propagation(X)\n",
    "\n",
    "    def loss_epochs(self):\n",
    "        \"\"\"Return the training and validation losses.\"\"\"\n",
    "        return np.array(self.train_losses), np.array(self.val_losses)\n",
    "\n",
    "# Example usage\n",
    "# Ensure you have the preprocessed data as numpy arrays\n",
    "# X_train_np, y_train_np, X_test_np, y_test should be numpy arrays\n",
    "\n",
    "# If you have pandas DataFrame/Series, convert them to numpy arrays\n",
    "X_train_np = X_train.to_numpy().T if hasattr(X_train, 'to_numpy') else X_train.T\n",
    "y_train_np = y_train.to_numpy().reshape(1, -1) if hasattr(y_train, 'to_numpy') else y_train.reshape(1, -1)\n",
    "X_test_np = X_test.to_numpy().T if hasattr(X_test, 'to_numpy') else X_test.T\n",
    "\n",
    "layers = [X_train_np.shape[0], 9, 5, 1]\n",
    "nn = NeuralNet(layers, epochs=1000, learning_rate=0.01, activation='sigmoid', validation_split=0.2, patience=10, batch_size=32)\n",
    "\n",
    "# Train the neural network\n",
    "nn.fit(X_train_np, y_train_np)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_nn = nn.predict(X_test_np)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_nn = mean_squared_error(y_test, y_pred_nn.T)\n",
    "print(f'Mean Squared Error for BP: {mse_nn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Ensure data is in numpy array format\n",
    "X_train = X_train.to_numpy() if hasattr(X_train, 'to_numpy') else X_train\n",
    "y_train = y_train.to_numpy() if hasattr(y_train, 'to_numpy') else y_train\n",
    "X_test = X_test.to_numpy() if hasattr(X_test, 'to_numpy') else X_test\n",
    "y_test = y_test.to_numpy() if hasattr(y_test, 'to_numpy') else y_test\n",
    "\n",
    "# Convert data to torch tensors\n",
    "X_train_tensor = torch.Tensor(X_train)\n",
    "y_train_tensor = torch.Tensor(y_train).view(-1, 1)\n",
    "X_test_tensor = torch.Tensor(X_test)\n",
    "y_test_tensor = torch.Tensor(y_test).view(-1, 1)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define the neural network\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(X_train.shape[1], 9)\n",
    "        self.fc2 = nn.Linear(9, 5)\n",
    "        self.fc3 = nn.Linear(5, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleNet()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "# Make predictions\n",
    "y_pred_bp_f = model(X_test_tensor).detach().numpy()\n",
    "\n",
    "# Evaluate the model\n",
    "mse_bp_f = mean_squared_error(y_test, y_pred_bp_f)\n",
    "print(f'Mean Squared Error for BP-F: {mse_bp_f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
