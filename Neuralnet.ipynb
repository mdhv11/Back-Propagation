{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age                    0\n",
      "sex                    0\n",
      "chest pain type        0\n",
      "resting bp s           0\n",
      "cholesterol            0\n",
      "fasting blood sugar    0\n",
      "resting ecg            0\n",
      "max heart rate         0\n",
      "exercise angina        0\n",
      "oldpeak                0\n",
      "ST slope               0\n",
      "target                 0\n",
      "dtype: int64\n",
      "   age  sex  chest pain type  resting bp s  cholesterol  fasting blood sugar  \\\n",
      "0   40    1                2           140          289                    0   \n",
      "1   49    0                3           160          180                    0   \n",
      "2   37    1                2           130          283                    0   \n",
      "3   48    0                4           138          214                    0   \n",
      "4   54    1                3           150          195                    0   \n",
      "\n",
      "   resting ecg  max heart rate  exercise angina  oldpeak  ST slope  target  \n",
      "0            0             172                0      0.0         1       0  \n",
      "1            0             156                0      1.0         2       1  \n",
      "2            1              98                0      0.0         1       0  \n",
      "3            0             108                1      1.5         2       1  \n",
      "4            0             122                0      0.0         1       0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Loading the dataset\n",
    "data = pd.read_csv('heart_statlog_cleveland_hungary_final.csv')\n",
    "\n",
    "# Checking for missing values\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Fill missing values with the mean of the column\n",
    "data.fillna(data.mean(), inplace=True)\n",
    "\n",
    "# Display the first few rows of the dataset to understand its structure\n",
    "print(data.head())\n",
    "\n",
    "# Assuming 'sex', 'chest pain type', 'fasting blood sugar', 'resting ecg', 'exercise angina', and 'ST slope' are categorical\n",
    "categorical_columns = ['sex', 'chest pain type', 'fasting blood sugar', 'resting ecg', 'exercise angina', 'ST slope']\n",
    "\n",
    "# Converting categorical columns to numerical format using one-hot encoding\n",
    "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Separating features and target variable\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error for MLR-F: 0.10639583725280026\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Initialize the model\n",
    "mlr = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "mlr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_mlr = mlr.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_mlr = mean_squared_error(y_test, y_pred_mlr)\n",
    "print(f'Mean Squared Error for MLR-F: {mse_mlr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 0.2501240888908279, Validation Loss: 0.2499222391603952\n",
      "Epoch 100, Train Loss: 0.24907978297505567, Validation Loss: 0.2511613845563069\n",
      "Epoch 200, Train Loss: 0.24884141432225404, Validation Loss: 0.2520564440057993\n",
      "Epoch 300, Train Loss: 0.24879746491477917, Validation Loss: 0.2524976075075723\n",
      "Epoch 400, Train Loss: 0.24878929822937115, Validation Loss: 0.25269781328873675\n",
      "Epoch 500, Train Loss: 0.24878774452427616, Validation Loss: 0.2527859426165189\n",
      "Epoch 600, Train Loss: 0.2487874156013423, Validation Loss: 0.25282427040848593\n",
      "Epoch 700, Train Loss: 0.24878731371389703, Validation Loss: 0.2528408645345218\n",
      "Epoch 800, Train Loss: 0.24878725387772171, Validation Loss: 0.2528480449607429\n",
      "Epoch 900, Train Loss: 0.24878720175478183, Validation Loss: 0.25285116100263966\n",
      "Mean Squared Error for BP: 0.24770101220831583\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNet:\n",
    "    def __init__(self, layers, epochs=1000, learning_rate=0.01, momentum=0.9, activation='sigmoid', validation_split=0.2):\n",
    "        self.L = len(layers)\n",
    "        self.n = layers.copy()\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.activation = activation\n",
    "        self.validation_split = validation_split\n",
    "\n",
    "        self.xi = [np.zeros(layers[lay]) for lay in range(self.L)]\n",
    "        self.w = [np.zeros((1, 1))]\n",
    "        for lay in range(1, self.L):\n",
    "            self.w.append(np.random.randn(layers[lay], layers[lay - 1]) * 0.01)\n",
    "        self.velocity = [np.zeros_like(w) for w in self.w]\n",
    "\n",
    "        self.activation_function = self.get_activation_function(activation)\n",
    "        self.activation_derivative = self.get_activation_derivative(activation)\n",
    "\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def get_activation_function(self, name):\n",
    "        if name == 'sigmoid':\n",
    "            return lambda z: 1 / (1 + np.exp(-z))\n",
    "        elif name == 'relu':\n",
    "            return lambda z: np.maximum(0, z)\n",
    "        elif name == 'linear':\n",
    "            return lambda z: z\n",
    "        elif name == 'tanh':\n",
    "            return lambda z: np.tanh(z)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "    def get_activation_derivative(self, name):\n",
    "        if name == 'sigmoid':\n",
    "            return lambda z: z * (1 - z)\n",
    "        elif name == 'relu':\n",
    "            return lambda z: np.where(z > 0, 1, 0)\n",
    "        elif name == 'linear':\n",
    "            return lambda z: np.ones_like(z)\n",
    "        elif name == 'tanh':\n",
    "            return lambda z: 1 - z**2\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        self.xi[0] = X\n",
    "        for lay in range(1, self.L):\n",
    "            self.xi[lay] = self.activation_function(np.dot(self.w[lay], self.xi[lay - 1]))\n",
    "        return self.xi[-1]\n",
    "\n",
    "    def backward_propagation(self, X, y):\n",
    "        m = y.shape[1]  # Number of samples\n",
    "        self.forward_propagation(X)\n",
    "        deltas = [0] * self.L\n",
    "        deltas[-1] = (self.xi[-1] - y) * self.activation_derivative(self.xi[-1])\n",
    "\n",
    "        for lay in range(self.L - 2, 0, -1):\n",
    "            deltas[lay] = np.dot(self.w[lay + 1].T, deltas[lay + 1]) * self.activation_derivative(self.xi[lay])\n",
    "\n",
    "        for lay in range(1, self.L):\n",
    "            self.velocity[lay] = self.momentum * self.velocity[lay] - self.learning_rate * np.dot(deltas[lay], self.xi[lay - 1].T) / m\n",
    "            self.w[lay] += self.velocity[lay]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Split the data into training and validation sets\n",
    "        split = int((1 - self.validation_split) * X.shape[1])\n",
    "        X_train, X_val = X[:, :split], X[:, split:]\n",
    "        y_train, y_val = y[:, :split], y[:, split:]\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            self.backward_propagation(X_train, y_train)\n",
    "            if epoch % 100 == 0:\n",
    "                train_loss = np.mean((self.xi[-1] - y_train) ** 2)\n",
    "                val_loss = np.mean((self.forward_propagation(X_val) - y_val) ** 2)\n",
    "                self.train_losses.append(train_loss)\n",
    "                self.val_losses.append(val_loss)\n",
    "                print(f'Epoch {epoch}, Train Loss: {train_loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.forward_propagation(X)\n",
    "\n",
    "    def loss_epochs(self):\n",
    "        return np.array(self.train_losses), np.array(self.val_losses)\n",
    "\n",
    "# Example usage\n",
    "layers = [X_train.shape[1], 9, 5, 1]\n",
    "nn = NeuralNet(layers, epochs=1000, learning_rate=0.01, activation='sigmoid', validation_split=0.2)\n",
    "\n",
    "# Train the neural network\n",
    "nn.fit(X_train.T, y_train.values.reshape(1, -1))\n",
    "\n",
    "# Make predictions\n",
    "y_pred_nn = nn.predict(X_test.T)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_nn = mean_squared_error(y_test, y_pred_nn.T)\n",
    "print(f'Mean Squared Error for BP: {mse_nn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not determine the shape of object type 'Series'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m y_train_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(y_train)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      9\u001b[0m X_test_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(X_test)\n\u001b[1;32m---> 10\u001b[0m y_test_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     12\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m TensorDataset(X_train_tensor, y_train_tensor)\n\u001b[0;32m     13\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: could not determine the shape of object type 'Series'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Convert data to torch tensors\n",
    "X_train_tensor = torch.Tensor(X_train)\n",
    "y_train_tensor = torch.Tensor(y_train).view(-1, 1)\n",
    "X_test_tensor = torch.Tensor(X_test)\n",
    "y_test_tensor = torch.Tensor(y_test).view(-1, 1)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define the neural network\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(X_train.shape[1], 9)\n",
    "        self.fc2 = nn.Linear(9, 5)\n",
    "        self.fc3 = nn.Linear(5, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleNet()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "# Make predictions\n",
    "y_pred_bp_f = model(X_test_tensor).detach().numpy()\n",
    "\n",
    "# Evaluate the model\n",
    "mse_bp_f = mean_squared_error(y_test, y_pred_bp_f)\n",
    "print(f'Mean Squared Error for BP-F: {mse_bp_f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
